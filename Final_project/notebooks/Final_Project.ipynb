{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Multiclass Logistic Regression on the UCI Digits Dataset\n", "\n", "This notebook is part of the AF3 Product Integrator.  \n", "The idea is to build a complete supervised learning workflow using the **Digits** dataset:\n", "- Load a real public dataset\n", "- Explore and clean the data\n", "- Apply preprocessing and normalization\n", "- Train a **multinomial Logistic Regression** model\n", "- Evaluate the results with proper metrics and plots\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Imports"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "\n", "from sklearn.datasets import load_digits\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.metrics import (\n", "    accuracy_score,\n", "    precision_score,\n", "    recall_score,\n", "    f1_score,\n", "    confusion_matrix,\n", "    classification_report\n", ")\n", "\n", "# Just to display all columns when needed\n", "pd.set_option(\"display.max_columns\", None)\n", "sns.set(style=\"whitegrid\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Load dataset from sklearn (UCI Digits)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Loading the digits dataset from sklearn\n", "digits = load_digits()\n", "\n", "# digits.data has the 64 numeric features, digits.target has the labels (0-9)\n", "X = pd.DataFrame(digits.data)\n", "y = pd.Series(digits.target, name=\"target\")\n", "\n", "# Combine into a single DataFrame for easier analysis\n", "df = pd.concat([X, y], axis=1)\n", "\n", "df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Basic exploratory data analysis (EDA)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Shape of the dataset\n", "print(\"Shape:\", df.shape)\n", "\n", "# Info about data types and non-null values\n", "df.info()"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Basic statistics for the numeric features\n", "df.describe().T.head(10)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.1 Missing values check"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Checking for missing values in each column\n", "df.isna().sum()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.2 Target class distribution"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["plt.figure(figsize=(6,4))\n", "sns.countplot(x=\"target\", data=df)\n", "plt.title(\"Digits class distribution (0-9)\")\n", "plt.xlabel(\"Digit\")\n", "plt.ylabel(\"Count\")\n", "plt.tight_layout()\n", "\n", "# Create figures folder if it does not exist\n", "import os\n", "os.makedirs(\"../figures\", exist_ok=True)\n", "plt.savefig(\"../figures/class_distribution.png\", dpi=300)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.3 Visualizing some sample digits"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Plotting some sample images to understand the data\n", "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n", "axes = axes.ravel()\n", "\n", "for i in range(10):\n", "    axes[i].imshow(digits.images[i], cmap=\"gray\")\n", "    axes[i].set_title(f\"Label: {digits.target[i]}\")\n", "    axes[i].axis(\"off\")\n", "\n", "plt.tight_layout()\n", "plt.savefig(\"../figures/sample_digits.png\", dpi=300)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.4 (Optional) Save dataset as CSV"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# This is optional, but useful to have a copy of the dataset in /data\n", "os.makedirs(\"../data\", exist_ok=True)\n", "df.to_csv(\"../data/digits.csv\", index=False)\n", "print(\"Saved digits.csv to ../data/digits.csv\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Feature/target split"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Separating features (X) and target (y)\n", "X = df.drop(columns=[\"target\"])\n", "y = df[\"target\"]\n", "\n", "print(\"Features shape:\", X.shape)\n", "print(\"Target shape:\", y.shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5. Train/Test split (70/30)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["X_train, X_test, y_train, y_test = train_test_split(\n", "    X, y, test_size=0.30, random_state=42, stratify=y\n", ")\n", "\n", "print(\"X_train:\", X_train.shape)\n", "print(\"X_test:\", X_test.shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6. Feature scaling (standardization)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# StandardScaler will normalize each feature to have mean ~0 and std ~1\n", "scaler = StandardScaler()\n", "\n", "X_train_scaled = scaler.fit_transform(X_train)\n", "X_test_scaled = scaler.transform(X_test)\n", "\n", "X_train_scaled[:3]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 7. Multiclass Logistic Regression (multinomial)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Creating the multinomial Logistic Regression model\n", "log_reg = LogisticRegression(\n", "    multi_class=\"multinomial\",\n", "    solver=\"lbfgs\",\n", "    max_iter=2000\n", ")\n", "\n", "# Training the model\n", "log_reg.fit(X_train_scaled, y_train)\n", "\n", "# Predictions\n", "y_pred = log_reg.predict(X_test_scaled)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 8. Evaluation metrics"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Computing the main metrics\n", "acc = accuracy_score(y_test, y_pred)\n", "prec = precision_score(y_test, y_pred, average=\"weighted\")\n", "rec = recall_score(y_test, y_pred, average=\"weighted\")\n", "f1 = f1_score(y_test, y_pred, average=\"weighted\")\n", "\n", "print(\"Accuracy:\", acc)\n", "print(\"Precision:\", prec)\n", "print(\"Recall:\", rec)\n", "print(\"F1-score:\", f1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 8.1 Save metrics and classification report"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Creating the results folder if it does not exist\n", "os.makedirs(\"../results\", exist_ok=True)\n", "\n", "# Saving a simple metrics summary\n", "metrics_text = (\n", "    f\"Accuracy: {acc}\\n\"\n", "    f\"Precision (weighted): {prec}\\n\"\n", "    f\"Recall (weighted): {rec}\\n\"\n", "    f\"F1-score (weighted): {f1}\\n\"\n", ")\n", "\n", "with open(\"../results/metrics.txt\", \"w\") as f:\n", "    f.write(metrics_text)\n", "\n", "print(\"Metrics saved to ../results/metrics.txt\")"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Full classification report (per class)\n", "report = classification_report(y_test, y_pred)\n", "print(report)\n", "\n", "with open(\"../results/classification_report.txt\", \"w\") as f:\n", "    f.write(report)\n", "\n", "print(\"Classification report saved to ../results/classification_report.txt\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 9. Confusion matrix"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["cm = confusion_matrix(y_test, y_pred)\n", "\n", "# Save confusion matrix as CSV as well (for the report if needed)\n", "cm_df = pd.DataFrame(cm, index=range(10), columns=range(10))\n", "cm_df.to_csv(\"../results/confusion_matrix.csv\", index=True)\n", "\n", "plt.figure(figsize=(8,6))\n", "sns.heatmap(cm, annot=True, fmt=\"g\", cmap=\"Blues\")\n", "plt.title(\"Confusion Matrix\")\n", "plt.xlabel(\"Predicted\")\n", "plt.ylabel(\"Actual\")\n", "plt.tight_layout()\n", "plt.savefig(\"../figures/confusion_matrix.png\", dpi=300)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 10. Simple coefficient analysis"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# The model learns one set of coefficients per class\n", "coefs = log_reg.coef_\n", "print(\"Coefficients shape:\", coefs.shape)  # (10 classes, 64 features)\n", "\n", "# Just to get an idea, we can look at the first row (class 0)\n", "coef_class_0 = pd.Series(coefs[0], index=X.columns)\n", "print(coef_class_0.sort_values(ascending=False).head(10))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 11. Short conclusions (for the report)\n", "\n", "In this notebook we:\n", "- Used a **real, public dataset** (UCI Digits via sklearn)\n", "- Performed a basic exploratory data analysis (EDA)\n", "- Verified there were no missing values\n", "- Standardized all numeric features\n", "- Trained a **multinomial Logistic Regression** model\n", "- Evaluated it using Accuracy, Precision, Recall and F1-score\n", "- Visualized the confusion matrix and inspected the learned coefficients\n", "\n", "These elements can be used directly in the final PDF report: methodology, results, and discussion.\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 5}